{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d3f7ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0871c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c97e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822222b0",
   "metadata": {},
   "source": [
    "The \\u0085 unicode character in some of the text files was causing the files to be split into multiple rows. So I need to remove this character from the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e08506dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dirs = ['data/aclImdb/train/pos', 'data/aclImdb/train/neg', 'data/aclImdb/test/pos', 'data/aclImdb/test/neg']\n",
    "for dir in dirs:\n",
    "    for dir_file in os.listdir(dir):\n",
    "        # rename or store the new file name for later rename\n",
    "    # Read in the file\n",
    "        with open(dir + '/' + dir_file, 'r') as file:\n",
    "    #         length = len(file.readlines())\n",
    "    #         if length > 1:\n",
    "    #             print(length)\n",
    "            filedata = file.read()\n",
    "    #         if '\\u0085' in filedata:\n",
    "    #             print(dir_file + '\\n' + filedata)\n",
    "\n",
    "    # Replace the target string\n",
    "\n",
    "        filedata = filedata.replace('\\u0085', ' ')\n",
    "\n",
    "    # Write the file out again\n",
    "        with open(dir + '/' + dir_file, 'w') as file:\n",
    "            file.write(filedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b228b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dirs = ['data/aclImdb/train/pos', 'data/aclImdb/train/neg', 'data/aclImdb/test/pos', 'data/aclImdb/test/neg']\n",
    "for dir in dirs:\n",
    "    for dir_file in os.listdir(dir):\n",
    "            file_number, file_end = dir_file.split('_')\n",
    "            num = file_number.zfill(5)  # num is 5 characters long with leading 0\n",
    "\n",
    "            \n",
    "            new_file = \"{}_{}\".format(num, file_end)\n",
    "            os.rename(dir + '/' + dir_file, dir + '/' + new_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c343d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the files into ordered lists, one for train and one for test\n",
    "dirs = ['train','test']\n",
    "files = {}\n",
    "for dir in dirs:\n",
    "    files[dir] = []\n",
    "    subdirs = ['pos', 'neg']\n",
    "    for subdir in subdirs:\n",
    "        files_tmp = []\n",
    "        for dir_file in os.listdir('data/aclImdb/' + dir + '/' + subdir):\n",
    "            files_tmp += [dir_file]\n",
    "        files_tmp = sorted(files_tmp)\n",
    "        files[dir].extend(files_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7728409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get two lists of the ratings, taken from the filenames\n",
    "dirs = ['train','test']\n",
    "ratings = {}\n",
    "for dir in dirs:\n",
    "    ratings[dir] = []\n",
    "    for file in files[dir]:\n",
    "        rating = int(file.split('_')[1].split('.')[0])\n",
    "        ratings[dir].append(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "8b943d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_pos = np.array(ratings['test'][:12])\n",
    "ratings_neg = np.array(ratings['test'][12500:12512])\n",
    "ratings_np = np.concatenate((ratings_pos, ratings_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "a864292e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10,  7,  7, 10,  7,  7,  9,  9,  7,  7,  8,  2,  3,  3,  4,  4,\n",
       "        4,  3,  1,  2,  4,  3,  3])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1275425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import numpy as np\n",
    "mean = statistics.mean(ratings['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5739932e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.26076"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_ratings = np.array(ratings['train'])\n",
    "statistics.mean(abs(tr_ratings - mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93d0539e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25bb80255dd34edba3c5fbce32e98db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54643184c67049508e35da707792ac27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2f5596483a59ffb3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/mark/.cache/huggingface/datasets/text/default-2f5596483a59ffb3/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f40cbad57eb45de847d2e8e79d39d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b0ca108ae049a1b9ca8ef1ff8578ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/mark/.cache/huggingface/datasets/text/default-2f5596483a59ffb3/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43bd2600c0644e2be6ba2f1a2f2e3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"text\", data_files={'train': ['data/aclImdb/train/pos/*.txt', 'data/aclImdb/train/neg/*.txt'], 'test': ['data/aclImdb/test/pos/*.txt', 'data/aclImdb/test/neg/*.txt']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36cfe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data_rows = 128\n",
    "small_data = load_dataset(\"text\", data_files={'train': ['data/aclImdb/train/pos/*.txt', 'data/aclImdb/train/neg/*.txt'], 'test': ['data/aclImdb/test/pos/*.txt', 'data/aclImdb/test/neg/*.txt']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4b10b5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72cefec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "272f3795",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'] = data['train'].add_column(\"rating\", ratings['train'])\n",
    "data['test'] = data['test'].add_column(\"rating\", ratings['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6feb1c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 12:39:16.042114: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-10 12:39:16.042142: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bcf5c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d44b7fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets[s3] in /home/mark/anaconda3/lib/python3.8/site-packages (1.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mark/anaconda3/lib/python3.8/site-packages (from datasets[s3]) (1.20.1)\n",
      "Requirement already satisfied: pandas in /home/mark/anaconda3/lib/python3.8/site-packages (from datasets[s3]) (1.2.4)\n",
      "Requirement already satisfied: packaging in /home/mark/anaconda3/lib/python3.8/site-packages (from datasets[s3]) (21.2)\n",
      "Requirement already satisfied: aiohttp in /home/mark/anaconda3/lib/python3.8/site-packages (from datasets[s3]) (3.8.1)\n",
      "Requirement already satisfied: dill in /home/mark/anaconda3/lib/python3.8/site-packages (from datasets[s3]) (0.3.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/mark/anaconda3/lib/python3.8/site-packages (from datasets[s3]) (2021.11.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/mark/anaconda3/lib/python3.8/site-packages (from datasets[s3]) (0.2.1)\n",
      "Requirement already satisfied: multiprocess in /home/mark/anaconda3/lib/python3.8/site-packages (from datasets[s3]) (0.70.12.2)\n",
      "Requirement already satisfied: xxhash in /home/mark/anaconda3/lib/python3.8/site-packages (from datasets[s3]) (2.0.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/mark/anaconda3/lib/python3.8/site-packages (from datasets[s3]) (4.62.3)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /home/mark/anaconda3/lib/python3.8/site-packages (from datasets[s3]) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/mark/anaconda3/lib/python3.8/site-packages (from datasets[s3]) (2.25.1)\n",
      "Requirement already satisfied: s3fs in /home/mark/anaconda3/lib/python3.8/site-packages (from datasets[s3]) (0.4.2)\n",
      "Requirement already satisfied: boto3 in /home/mark/anaconda3/lib/python3.8/site-packages (from datasets[s3]) (1.20.20)\n",
      "Requirement already satisfied: botocore in /home/mark/anaconda3/lib/python3.8/site-packages (from datasets[s3]) (1.23.20)\n",
      "Requirement already satisfied: filelock in /home/mark/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets[s3]) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mark/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets[s3]) (3.7.4.3)\n",
      "Requirement already satisfied: pyyaml in /home/mark/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets[s3]) (5.4.1)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /home/mark/anaconda3/lib/python3.8/site-packages (from packaging->datasets[s3]) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/mark/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets[s3]) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/mark/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets[s3]) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mark/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets[s3]) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/mark/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets[s3]) (2.10)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/mark/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets[s3]) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/mark/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets[s3]) (5.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/mark/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets[s3]) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/mark/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets[s3]) (4.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/mark/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets[s3]) (20.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/mark/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets[s3]) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/mark/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets[s3]) (2.0.9)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/mark/anaconda3/lib/python3.8/site-packages (from boto3->datasets[s3]) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/mark/anaconda3/lib/python3.8/site-packages (from boto3->datasets[s3]) (0.5.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/mark/anaconda3/lib/python3.8/site-packages (from botocore->datasets[s3]) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/mark/anaconda3/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore->datasets[s3]) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/mark/anaconda3/lib/python3.8/site-packages (from pandas->datasets[s3]) (2021.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install datasets[s3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c07c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets.filesystems import S3FileSystem\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "796d4a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/.aws\n",
    "!cp ../aws_config ~/.aws/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ed9c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.session import Session\n",
    "\n",
    "s3_session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8333247",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = datasets.filesystems.S3FileSystem(session=s3_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a84d3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mlzoomcamp-capstone/dataset_dict.json',\n",
       " 'mlzoomcamp-capstone/test',\n",
       " 'mlzoomcamp-capstone/train']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.ls('mlzoomcamp-capstone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b938f560",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.save_to_disk('s3://mlzoomcamp-capstone', fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "05700511",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 256\n",
    "MAX_ROWS = 24\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "935fefef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'rating'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'rating'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_from_disk('s3://mlzoomcamp-capstone',fs=s3)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5526d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(\"This is a test\", truncation=True, padding=True, max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19027808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_subset(data, max_rows):\n",
    "    return data.filter(lambda e, i: (i in range(int(max_rows / 2)) or i in range(int(len(data['train']) / 2), int(len(data['train']) / 2) + int(max_rows / 2))), with_indices=True)\n",
    "\n",
    "def tokenize_function(example, tokenizer, batched=True):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=True, max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52ea3dfa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_ROWS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1248058/654190385.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msmaller_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_ROWS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'MAX_ROWS' is not defined"
     ]
    }
   ],
   "source": [
    "smaller_data = data_subset(data, MAX_ROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dd4e08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "586346bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dictionary = {'epochs': 2, 'lr': 0.0005, 'layer_dropout': 0, 'att_dropout': 0, 'max_length': 512, 'batch_size': 8, 'max_rows': 25000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f607d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60bed219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertModel, DistilBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6978faab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=True, max_length=params_dictionary['max_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26df94b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(params_dictionary):\n",
    "    max_rows = params_dictionary['max_rows']\n",
    "    max_length = params_dictionary['max_length']\n",
    "    smaller_data = data.filter(lambda e, i: (i in range(int(max_rows / 2)) or i in range(12500, 12500 + int(max_rows / 2))), with_indices=True)\n",
    "    tokenized_data = smaller_data.map(tokenize_function, batched=True)\n",
    "    print(tokenized_data['test'])\n",
    "    \n",
    "    tf_train_dataset = tokenized_data[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\"],\n",
    "    label_cols=[\"rating\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=params_dictionary['batch_size'],\n",
    "        )\n",
    "    tf_val_dataset = tokenized_data[\"test\"].to_tf_dataset(\n",
    "        columns=[\"attention_mask\", \"input_ids\"],\n",
    "        label_cols=[\"rating\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=params_dictionary['batch_size'],\n",
    "        )\n",
    "    return tf_train_dataset, tf_val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36030603",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1248058/2699065550.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_data' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5288477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_model(params_dictionary):\n",
    " \n",
    "    # Configure DistilBERT's initialization\n",
    "    config = DistilBertConfig(dropout=params_dictionary['layer_dropout'] , \n",
    "                              attention_dropout=params_dictionary['att_dropout'], \n",
    "                              output_hidden_states=True)\n",
    "\n",
    "    # The bare, pre-trained DistilBERT transformer model outputting raw hidden-states \n",
    "    # and without any specific head on top.\n",
    "    distilBERT = TFDistilBertModel.from_pretrained(checkpoint, config=config)\n",
    "\n",
    "    # Make DistilBERT layers untrainable\n",
    "    for layer in distilBERT.layers:\n",
    "        layer.trainable = False\n",
    "    return distilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05fb3864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, params_dictionary):\n",
    "    \"\"\"\n",
    "    Template for building a model off of the BERT or DistilBERT architecture\n",
    "    for a binary classification task.\n",
    "    \n",
    "    Input:\n",
    "      - transformer:  a base Hugging Face transformer model object (BERT or DistilBERT)\n",
    "                      with no added classification head attached.\n",
    "      - max_length:   integer controlling the maximum number of encoded tokens \n",
    "                      in a given sequence.\n",
    "    \n",
    "    Output:\n",
    "      - model:        a compiled tf.keras.Model with added classification layers \n",
    "                      on top of the base pre-trained model architecture.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define weight initializer with a random seed to ensure reproducibility\n",
    "    weight_initializer = tf.keras.initializers.GlorotNormal(seed=RANDOM_STATE) \n",
    "    \n",
    "    # Define input layers\n",
    "    input_ids_layer = tf.keras.layers.Input(shape=(params_dictionary['max_length'],), \n",
    "                                            name='input_ids', \n",
    "                                            dtype='int32')\n",
    "    input_attention_layer = tf.keras.layers.Input(shape=(params_dictionary['max_length'],), \n",
    "                                                  name='attention_mask', \n",
    "                                                  dtype='int32')\n",
    "    \n",
    "    # DistilBERT outputs a tuple where the first element at index 0\n",
    "    # represents the hidden-state at the output of the model's last layer.\n",
    "    # It is a tf.Tensor of shape (batch_size, sequence_length, hidden_size=768).\n",
    "    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]\n",
    "    \n",
    "    # We only care about DistilBERT's output for the [CLS] token, \n",
    "    # which is located at index 0 of every encoded sequence.  \n",
    "    # Splicing out the [CLS] tokens gives us 2D data.\n",
    "    # cls_token = last_hidden_state[:, 0, :]\n",
    "    \n",
    "    ##                                                 ##\n",
    "    ## Define additional dropout and dense layers here ##\n",
    "    ##                                            \n",
    "    \n",
    "    # vector = tf.reshape(last_hidden_state, [-1])\n",
    "\n",
    "    vector = tf.keras.layers.GlobalAvgPool1D()(last_hidden_state)\n",
    "\n",
    "    output1 = tf.keras.layers.Dense(128, \n",
    "                                   activation='relu',\n",
    "                                   kernel_initializer=weight_initializer,  \n",
    "                                   kernel_constraint=None,\n",
    "                                   bias_initializer='zeros'\n",
    "                                   )(vector)\n",
    "    \n",
    "    output2 = tf.keras.layers.Dense(128, \n",
    "                                   activation='relu',\n",
    "                                   kernel_initializer=weight_initializer,  \n",
    "                                   kernel_constraint=None,\n",
    "                                   bias_initializer='zeros'\n",
    "                                   )(output1)\n",
    "    \n",
    "    output3 = tf.keras.layers.Dense(64, \n",
    "                                   activation='relu',\n",
    "                                   kernel_initializer=weight_initializer,  \n",
    "                                   kernel_constraint=None,\n",
    "                                   bias_initializer='zeros'\n",
    "                                   )(output2)\n",
    "\n",
    "    output4 = tf.keras.layers.Dense(10, \n",
    "                                   activation='relu',\n",
    "                                   kernel_initializer=weight_initializer,  \n",
    "                                   kernel_constraint=None,\n",
    "                                   bias_initializer='zeros'\n",
    "                                   )(output3)\n",
    "    \n",
    "    # Define a single node that makes up the output layer (for binary classification)\n",
    "    output5 = tf.keras.layers.Dense(1, \n",
    "                                   activation=None,\n",
    "                                   kernel_initializer=weight_initializer,  \n",
    "                                   kernel_constraint=None,\n",
    "                                   bias_initializer='zeros'\n",
    "                                   )(output4)\n",
    "    \n",
    "    # Define the model\n",
    "    model = tf.keras.Model([input_ids_layer, input_attention_layer], output5)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=params_dictionary['lr']), \n",
    "                  loss=MeanSquaredError(),\n",
    "                  metrics=['mse'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee7241ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 1\n",
    "from tensorflow.keras.losses import MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a685327b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 12:39:46.716578: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-10 12:39:46.716659: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-10 12:39:46.716705: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mark-laptop): /proc/driver/nvidia/version does not exist\n",
      "2021-12-10 12:39:46.717070: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-10 12:39:46.739568: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2021-12-10 12:39:46.783876: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2021-12-10 12:39:47.001444: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2021-12-10 12:39:47.056216: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFDistilBertModel: ['mlm___cls', 'nsp___cls', 'bert']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['distilbert']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "distilBERT = config_model(params_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "307e4e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilBERT_model = build_model(distilBERT, params_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d73cb8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  66362880   ['input_ids[0][0]',              \n",
      " BertModel)                     ast_hidden_state=(N               'attention_mask[0][0]']         \n",
      "                                one, 512, 768),                                                   \n",
      "                                 hidden_states=((No                                               \n",
      "                                ne, 512, 768),                                                    \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768)),                                               \n",
      "                                 attentions=None)                                                 \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 768)         0           ['tf_distil_bert_model[0][7]']   \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          98432       ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          16512       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           8256        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 10)           650         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            11          ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,486,741\n",
      "Trainable params: 123,861\n",
      "Non-trainable params: 66,362,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "distilBERT_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b83150e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 12:40:01.742626: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2021-12-10 12:40:01.918641: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "distilBERT_model.load_weights('provisional_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5d503008",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dictionary = {'epochs': 2, 'lr': 0.0005, 'layer_dropout': 0, 'att_dropout': 0, 'max_length': 512, 'batch_size': 8, 'max_rows': 24}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d98c2f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/tmp9p8uibbe/mlzoomcamp-capstone/train/cache-25f4c7c6fd533119.arrow\n",
      "Loading cached processed dataset at /tmp/tmp9p8uibbe/mlzoomcamp-capstone/test/cache-e8f5c4ab115d2066.arrow\n",
      "Loading cached processed dataset at /tmp/tmp9p8uibbe/mlzoomcamp-capstone/train/cache-6164583956468d4b.arrow\n",
      "Loading cached processed dataset at /tmp/tmp9p8uibbe/mlzoomcamp-capstone/test/cache-71db71ced1e00a8e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'rating', 'text', 'token_type_ids'],\n",
      "    num_rows: 25000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tf_train_dataset, tf_val_dataset = prepare_data(params_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "d3db8f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = distilBERT_model.predict(tf_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42a645d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ({attention_mask: (None, None), input_ids: (None, None)}, (None,)), types: ({attention_mask: tf.int64, input_ids: tf.int64}, tf.int64)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "1ba124ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.2616544"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(predictions[:12]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "4a87df80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.2218375"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(predictions[12:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "d1115566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.972178 ],\n",
       "       [10.159443 ],\n",
       "       [ 3.959504 ],\n",
       "       [ 6.376056 ],\n",
       "       [ 8.9968405],\n",
       "       [ 5.8000817],\n",
       "       [ 7.109173 ],\n",
       "       [ 9.432082 ],\n",
       "       [ 2.8598983],\n",
       "       [ 8.451174 ],\n",
       "       [ 7.4948816],\n",
       "       [ 7.528542 ],\n",
       "       [ 2.6925578],\n",
       "       [ 4.1040683],\n",
       "       [ 2.3468995],\n",
       "       [ 8.318647 ],\n",
       "       [ 2.6734893],\n",
       "       [ 6.346175 ],\n",
       "       [ 6.707441 ],\n",
       "       [ 3.2532237],\n",
       "       [ 3.3133276],\n",
       "       [ 4.32001  ],\n",
       "       [ 3.2285888],\n",
       "       [ 3.3576155]], dtype=float32)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f762ac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 21s 5s/step - loss: 4.2710 - mse: 4.2710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.270998954772949, 4.270998954772949]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilBERT_model.evaluate(tf_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "1215b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_np = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "78ea704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = predictions_np[:,0] - ratings_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "d4a05a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.02782154,  0.1594429 , -3.04049611, -0.62394381, -1.00315952,\n",
       "       -1.19991827,  0.10917282,  0.43208218, -6.14010167,  1.45117378,\n",
       "        0.49488163, -0.47145796,  0.69255781,  1.10406828, -0.65310049,\n",
       "        4.31864738, -1.32651067,  2.34617519,  3.70744085,  2.25322366,\n",
       "        1.31332755,  0.32001019,  0.22858882,  0.35761547])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "66385d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.270999236775239"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(diff ** 2)/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "96bf7eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10,  7,  7, 10,  7,  7,  9,  9,  7,  7,  8,  2,  3,  3,  4,  4,\n",
       "        4,  3,  1,  2,  4,  3,  3])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "d5fa721d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tflite_runtime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-267-e5a240a74662>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtflite_runtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpreter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtflite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tflite_runtime'"
     ]
    }
   ],
   "source": [
    "import tflite_runtime.interpreter as tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59f1648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
